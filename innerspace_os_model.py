# -*- coding: utf-8 -*-
"""InnerSpace OS model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ovw18kfPlD8kPtxm1iTCPXm8aO_jxgTZ

Install lib
"""

!pip install -q transformers==4.40.1 scikit-learn

"""Step 1: Upload Excel File"""

from google.colab import files
uploaded = files.upload()  # Upload your .xlsx file manually

"""Step 2: Load + preprocess"""

import pandas as pd
from io import BytesIO

# âœ… Correct way: get the file content, not just the name
file_name = next(iter(uploaded))
df = pd.read_csv(BytesIO(uploaded[file_name]))

# Continue with cleaning
df = df[['clean_text','mood_label']].dropna()
df.columns = ['text','label']
df['label'] = df['label'].apply(lambda x: 1 if x.strip().lower() == 'good' else 0)

df.head()

"""Step 3: Train-Test Split + tokenise"""

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["text"].tolist(),
    df["label"].tolist(),
    test_size=0.2,
    stratify=df["label"],
    random_state=42
)

from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

""" Step 4: Create Dataset"""

import torch

class MoodDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = MoodDataset(train_encodings, train_labels)
val_dataset = MoodDataset(val_encodings, val_labels)

"""Step 5: Load Model + Use Basic .fit():"""

import torch
from transformers import DistilBertForSequenceClassification, AdamW
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# DataLoader
tr_ds = MoodDataset(train_enc, train_l.tolist())
vl_ds = MoodDataset(val_enc, val_l.tolist())
tr_loader = torch.utils.data.DataLoader(tr_ds, batch_size=8, shuffle=True)
vl_loader = torch.utils.data.DataLoader(vl_ds, batch_size=8)

# Optimizer & train loop
optim = AdamW(model.parameters(), lr=2e-5)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(3):
    model.train()
    for batch in tr_loader:
        batch = {k:v.to(device) for k,v in batch.items()}
        loss = model(**batch).loss
        loss.backward()
        optim.step()
        optim.zero_grad()
    print("Epoch", epoch, "done.")

"""Step 6: Evaluate Metrics Manually:"""

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
model.eval()
all_preds, all_labels = [], []
for batch in vl_loader:
    inputs = {k:v.to(device) for k,v in batch.items()}
    logits = model(**inputs).logits.detach().cpu().numpy()
    all_preds += np.argmax(logits, axis=1).tolist()
    all_labels += inputs['labels'].cpu().numpy().tolist()

acc = accuracy_score(all_labels, all_preds)
prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')
print({"accuracy":acc,"precision":prec,"recall":rec,"f1":f1})

"""Step 7: Save model"""

model.save_pretrained("binary_model")
tokenizer.save_pretrained("binary_model")
!zip -r model.zip binary_model
files.download("model.zip")